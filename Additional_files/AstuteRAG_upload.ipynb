{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06520432-a156-41ab-8e6b-a34dbc8eb9b5",
   "metadata": {},
   "source": [
    "## This code is an exact implementation of a research paper on Astute RAG (Retrieval-Augmented Generation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7784b-89c9-4e09-9851-efb378c60a7f",
   "metadata": {},
   "source": [
    "## https://arxiv.org/pdf/2410.07176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764047c-c71f-48db-a9d8-4ebf15f18128",
   "metadata": {},
   "source": [
    "## ASTUTE RAG\r\n",
    "\r\n",
    "The Astute RAG method is designed to address issues in Retrieval-Augmented Generation (RAG) where retrieval from external sources may provide unreliable or misleading information. This leads to knowledge conflicts with an LLM’s internal knowledge, reducing answer reliability.\r\n",
    "Astute RAG takes three key steps to improve RAG’s performance by integrating internal and external information selectively and iteratively to filter out low-quality data:\r\n",
    "1. Adaptive Internal knowledge generation: The LLM generates passages from its internal knowledge based on the query. The LLM will have the ability to decide the number of passages to generate( with a predefined upper limit). These passages are appended into a list along with the passages retrieved from external sources.\r\n",
    "\r\n",
    "\r\n",
    "2. Iterative Source-aware Knowledge Consolidation: For each passage in the list, we additionally provide the source of generation for the LLM to check its reliability. The objective of this step is to iteratively improve the response quality over each iteration and produce fewer and refined passages. \r\n",
    "\r\n",
    "In each iteration, the LLM consolidates information by:\r\n",
    "Grouping Consistent Information: The LLM clusters passages that share similar details or answer elements, producing a consolidated, refined passage representing the consistent information.\r\n",
    "Identifying Conflicts: When passages disagree (e.g., they present different facts or perspectives), they are separated and labelled as conflicting. This way, the model can weigh each conflicting source’s reliability, rather than combining contradictory information.\r\n",
    "Filtering Irrelevant Information: Passages irrelevant to the query are excluded to avoid distractions or inaccuracies.\r\n",
    "\r\n",
    "\r\n",
    "The above steps repeat for a set number of iterations (with an upper bound parameter t ). Each cycle progressively refines the knowledge pool by further consolidating consistent data, reducing irrelevant information, and highlighting conflicts until the information pool is accurate, consistent, and minimal in contradictions.\r\n",
    "\r\n",
    "\r\n",
    "   3. Answer Finalisation: The refined passages from the above step are now used for the final answer generation.  For each group of passages (from the consolidated knowledge), the model generates potential answers, ensuring that different perspectives are considered separately. Each answer is assigned a confidence score based on factors like reliability of source and persistence across sources. The LLM evaluates all the proposed answers and selects the one with the highest confidence score as the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c524b-e3f2-4784-8650-1d7bb6192b51",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "This Python script implements a Retrieval-Augmented Generation (RAG) model for knowledge consolidation using external and internal document sources. It integrates OpenAI's GPT-3.5 model with LangChain to perform document search, generate internal knowledge passages, merge documents, and provide consolidated answers to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244b29a-9011-4ef7-9140-db582bf1155a",
   "metadata": {},
   "source": [
    "## Key Features:\n",
    "Document Retrieval: It uses TavilySearchResults to search external documents based on the query and retrieve relevant content.\n",
    "\n",
    "Internal Document Generation: It generates internal knowledge passages using OpenAI's GPT-3.5 model, based on an initial context.\n",
    "\n",
    "Document Merging: The script merges internal and external documents into a consolidated list.\n",
    "\n",
    "Knowledge Consolidation: It iteratively consolidates information, ensuring consistency and handling conflicting data.\n",
    "\n",
    "Answer Generation: Finally, the consolidated knowledge is used to generate an answer to the user's query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169141e1-783a-4881-bcbe-1ae6c8d2a0be",
   "metadata": {},
   "source": [
    "## Process Flow:\n",
    "Document Search: Retrieve external information related to the user’s query.\n",
    "\n",
    "Internal Passage Generation: Generate additional internal knowledge passages using GPT-3.5.\n",
    "\n",
    "Document Merging: Combine both internal and external documents into a single list.\n",
    "\n",
    "Consolidation: Use iterative calls to GPT-3.5 to consolidate information, resolving inconsistencies(Implementation of Astute Rag).\n",
    "\n",
    "Answer Generation: Generate a final, consolidated answer based on the merged and refined knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d4ef1-e189-4a12-bdc5-391345bb5841",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    "langchain_openai for OpenAI's integration with LangChain.\n",
    "\n",
    "langchain_community for TavilySearch and other community tools.\n",
    "\n",
    "OpenAI API Key for accessing GPT-3.5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2109fa47-6fc4-4991-b323-c5f57744c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-openai langchain-community openai tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2bfaf67-af58-4416-962f-8e26d8b2c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713f98b6-e898-426e-a375-2bd4686808a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"TAVILY_API_KEY\"] =\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51acd89d-b50a-4792-92bb-01900dfdb2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your query:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial merged_docs: [{'content': '- Hello, how are you today?', 'source': 'internal'}, {'content': \"- Hi there, what's on your mind?\", 'source': 'internal'}, {'content': \"- Hey, haven't heard from you in a while.\", 'source': 'internal'}, {'content': '- Hi, do you have a moment to chat?', 'source': 'internal'}, {'content': '- Hi friend, just checking in.', 'source': 'internal'}, {'content': 'H', 'source': 'External'}, {'content': 'T', 'source': 'External'}, {'content': 'T', 'source': 'External'}, {'content': 'P', 'source': 'External'}, {'content': 'E', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': 'o', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': '(', 'source': 'External'}, {'content': \"'\", 'source': 'External'}, {'content': '4', 'source': 'External'}, {'content': '0', 'source': 'External'}, {'content': '0', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'C', 'source': 'External'}, {'content': 'l', 'source': 'External'}, {'content': 'i', 'source': 'External'}, {'content': 'e', 'source': 'External'}, {'content': 'n', 'source': 'External'}, {'content': 't', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'E', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': 'o', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': ':', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'B', 'source': 'External'}, {'content': 'a', 'source': 'External'}, {'content': 'd', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'R', 'source': 'External'}, {'content': 'e', 'source': 'External'}, {'content': 'q', 'source': 'External'}, {'content': 'u', 'source': 'External'}, {'content': 'e', 'source': 'External'}, {'content': 's', 'source': 'External'}, {'content': 't', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'f', 'source': 'External'}, {'content': 'o', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'u', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': 'l', 'source': 'External'}, {'content': ':', 'source': 'External'}, {'content': ' ', 'source': 'External'}, {'content': 'h', 'source': 'External'}, {'content': 't', 'source': 'External'}, {'content': 't', 'source': 'External'}, {'content': 'p', 'source': 'External'}, {'content': 's', 'source': 'External'}, {'content': ':', 'source': 'External'}, {'content': '/', 'source': 'External'}, {'content': '/', 'source': 'External'}, {'content': 'a', 'source': 'External'}, {'content': 'p', 'source': 'External'}, {'content': 'i', 'source': 'External'}, {'content': '.', 'source': 'External'}, {'content': 't', 'source': 'External'}, {'content': 'a', 'source': 'External'}, {'content': 'v', 'source': 'External'}, {'content': 'i', 'source': 'External'}, {'content': 'l', 'source': 'External'}, {'content': 'y', 'source': 'External'}, {'content': '.', 'source': 'External'}, {'content': 'c', 'source': 'External'}, {'content': 'o', 'source': 'External'}, {'content': 'm', 'source': 'External'}, {'content': '/', 'source': 'External'}, {'content': 's', 'source': 'External'}, {'content': 'e', 'source': 'External'}, {'content': 'a', 'source': 'External'}, {'content': 'r', 'source': 'External'}, {'content': 'c', 'source': 'External'}, {'content': 'h', 'source': 'External'}, {'content': \"'\", 'source': 'External'}, {'content': ')', 'source': 'External'}]\n",
      "Iteration 1 Consolidated Info:\n",
      "There are several internal messages that all express greetings and interest in chatting, such as \"Hello, how are you today?\" and \"Hi friend, just checking in.\" However, the external messages do not provide relevant information to the query.\n",
      "\n",
      "Iteration 2 Consolidated Info:\n",
      "Consolidated Document:\n",
      "\n",
      "Internal messages express greetings and interest in chatting, such as \"Hello, how are you today?\" and \"Hi friend, just checking in.\" External messages do not provide relevant information to the query.\n",
      "\n",
      "Iteration 3 Consolidated Info:\n",
      "Consolidated Document: \n",
      "Internal messages contain expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" External messages do not contain pertinent information related to the query.\n",
      "\n",
      "Iteration 4 Consolidated Info:\n",
      "Consolidated Document: \n",
      "Internal messages from various sources include expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" The external messages do not provide relevant information related to the query.\n",
      "\n",
      "Iteration 5 Consolidated Info:\n",
      "Consolidated Document: \n",
      "Internal messages from various sources include expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" The external messages do not provide relevant information related to the query.\n",
      "\n",
      "Iteration 6 Consolidated Info:\n",
      "Consolidated Document: \n",
      "Internal messages from various sources include expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" The external messages do not provide relevant information related to the query.\n",
      "\n",
      "Iteration 7 Consolidated Info:\n",
      "Consolidated Document: \n",
      "\n",
      "Internal messages from various sources include expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" The external messages do not provide relevant information related to the query.\n",
      "\n",
      "Iteration 8 Consolidated Info:\n",
      "Consolidated Document: \n",
      "Internal messages from various sources include expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" The external messages do not provide relevant information related to the query.\n",
      "\n",
      "Iteration 9 Consolidated Info:\n",
      "Consolidated Document: \n",
      "Internal messages from various sources include expressions of greetings and interest in chatting, with phrases like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" The external messages do not provide relevant information related to the query. \n",
      "\n",
      "Conflicting Documents: \n",
      "\n",
      "Document 1: \n",
      "The message simply says \"hi\" without any additional context or information. \n",
      "[Source: Document 1]\n",
      "\n",
      "Iteration 10 Consolidated Info:\n",
      "Consolidated Document:\n",
      "\n",
      "Internal and external messages contain greetings like \"Hello, how are you today?\" and \"Hi friend, just checking in.\" One message simply says \"hi\" without additional context.\n",
      "\n",
      "Final Answer:\n",
      "['Based on the consolidated information, the word \"hi\" is a greeting without additional context provided. It is commonly used in messages to say hello or start a conversation.']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "from langchain import OpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
    "\n",
    "# Step 1: Define the search tool\n",
    "search = TavilySearchResults(max_results=2)\n",
    "\n",
    "# Function to perform the search and retrieve documents\n",
    "def retrieve_documents(query):\n",
    "    # Assuming search_results can have dictionaries or strings\n",
    "    search_results = search.invoke(query)  # Adjust this to however you're obtaining search results\n",
    "    external_docs = []\n",
    "    for result in search_results:\n",
    "        if isinstance(result, dict):\n",
    "            content = result.get(\"content\", \"No content available\")\n",
    "            source = result.get(\"source\", \"External\")  # Default source if not available\n",
    "        else:  # If result is a string, assume it's content with no source information\n",
    "            content = result\n",
    "            source = \"External\"\n",
    "        external_docs.append({\"content\": content, \"source\": source})\n",
    "    return external_docs\n",
    "\n",
    "# Step 2: Internal knowledge generation using OpenAI LLM\n",
    "# Function to generate internal passages\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "client = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Updated code for using chat completions\n",
    "from langchain_community.llms import OpenAI\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "def generate_internal_passages(initial_context):\n",
    "    prompt = f\"\"\"\n",
    "    Generate additional internal passages related to the provided context. \n",
    "    Use the following initial context for inspiration:\n",
    "    '{initial_context}'\n",
    "    Please provide concise passages that cover various relevant aspects based on the context given.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the new method to create a chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Extract the text from the response\n",
    "    internal_passages = response.choices[0].message.content.strip().split('\\n')\n",
    "    return internal_passages\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Merging internal and external documents\n",
    "def merge_documents(internal_docs, external_docs):\n",
    "    merged_docs = []\n",
    "    for doc in internal_docs:\n",
    "        if isinstance(doc, dict):  # Check if doc is a dictionary\n",
    "            merged_docs.append({\"content\": doc[\"content\"], \"source\": doc.get(\"source\", \"internal\")})\n",
    "        else:\n",
    "            merged_docs.append({\"content\": doc, \"source\": \"internal\"})  # Handle string case\n",
    "\n",
    "    for doc in external_docs:\n",
    "        merged_docs.append({\"content\": doc[\"content\"], \"source\": doc[\"source\"]})\n",
    "\n",
    "    return merged_docs\n",
    "\n",
    "# Step 4: Define the prompt for knowledge consolidation\n",
    "def create_consolidation_prompt(merged_docs, query):\n",
    "    merged_docs_str = \"\\n\".join([f\"{doc['content']} [Source: {doc['source']}]\" for doc in merged_docs])\n",
    "    return f\"\"\"\n",
    "    Task: Consolidate information from the provided documents in response to the given question.\n",
    "\n",
    "    * For documents that provide consistent information, cluster them together and summarize the key details into a single, concise document.\n",
    "    * For documents with conflicting information, separate them into distinct documents, ensuring each captures the unique perspective or data.\n",
    "    * Exclude any information irrelevant to the query.\n",
    "\n",
    "    For each new document created, clearly indicate:\n",
    "    * Whether the source was from memory or an external retrieval.\n",
    "    * The original document numbers for transparency.\n",
    "\n",
    "    Merged Documents: {merged_docs_str}\n",
    "    Question: {query}\n",
    "    New Context:\n",
    "    \"\"\"\n",
    "\n",
    "# Step 5: Call OpenAI's LLM to generate answers\n",
    "def generate_answer(consolidated_docs, query, initial_context):\n",
    "    context = \"\\n\".join([f\"{doc['source']}: {doc['content']}\" for doc in consolidated_docs])\n",
    "    prompt = f\"\"\"\n",
    "    Task: Answer the following question using the consolidated information from internal and external documents.\n",
    "\n",
    "    Initial Context: {initial_context}\n",
    "    Consolidated Context: {context}\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip().split('\\n')\n",
    "\n",
    "# Main function to execute the entire process\n",
    "def main():\n",
    "    # Ask the user for their query\n",
    "    query = input(\"Please enter your query: \")\n",
    "    initial_context = \"You are a helpful agent\"  # You can modify this as needed\n",
    "\n",
    "    # Step 1: Retrieve external documents\n",
    "    external_docs = retrieve_documents(query)\n",
    "    \n",
    "    # Step 2: Generate internal documents using OpenAI\n",
    "    internal_docs = generate_internal_passages(query)\n",
    "    \n",
    "    # Step 3: Merge internal and external documents\n",
    "    merged_docs = merge_documents(internal_docs, external_docs)  # Initialize merged_docs here\n",
    "    print(\"Initial merged_docs:\", merged_docs)  # Debug: Verify merged_docs initialization\n",
    "\n",
    "    # Step 4 & 5: Iteratively consolidate information\n",
    "    num_iterations = 10\n",
    "    \n",
    "    # Create an instance of OpenAI client\n",
    "    llm = client  # This should refer to the correct OpenAI instance, not `model`\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        merged_docs_str = \"\\n\".join([f\"{doc['content']} [Source: {doc['source']}]\" for doc in merged_docs])\n",
    "        \n",
    "        # Use the prompt consolidation here\n",
    "        consolidation_prompt = f\"\"\"\n",
    "        Task: Consolidate information from the provided documents in response to the given question.\n",
    "\n",
    "        * For documents that provide consistent information, cluster them together and summarize the key details into a single, concise document.\n",
    "        * For documents with conflicting information, separate them into distinct documents, ensuring each captures the unique perspective or data.\n",
    "        * Exclude any information irrelevant to the query.\n",
    "\n",
    "        Merged Documents: {merged_docs_str}\n",
    "        Question: {query}\n",
    "        New Context:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the LLM to consolidate information\n",
    "        response = llm.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"user\", \"content\": consolidation_prompt}],\n",
    "            max_tokens=150  # Adjust max_tokens as needed\n",
    "        )\n",
    "        \n",
    "        consolidated_info = response.choices[0].message.content.strip()\n",
    "        print(f\"Iteration {i + 1} Consolidated Info:\\n{consolidated_info}\\n\")\n",
    "        \n",
    "        # Update the merged_docs for the next iteration\n",
    "        merged_docs = [{\"content\": line.strip(), \"source\": \"Consolidated\"} for line in consolidated_info.split('\\n') if line.strip()]\n",
    "\n",
    "    # Final answer generation\n",
    "    final_answer = generate_answer(merged_docs, query, initial_context)\n",
    "    \n",
    "    # Print the final answer\n",
    "    print(f\"Final Answer:\\n{final_answer}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c8c1b-f5e9-4403-b305-8242aaf6a7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f527b-b618-4ce1-a984-249119c9e15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00295983-d60a-41f0-9ccc-771785edb265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
